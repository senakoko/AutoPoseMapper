from tensorflow import keras as k
import tensorflow as tf
import os


class Sampling(k.layers.Layer):
    """
    Samples a codings vector from the Normal distribution with mean μ and standard deviation σ.

    It takes the mean and log variances from the encoded layer and generates random sampling.

    Returns
    -------
    It outputs the codings layers
    """

    def call(self, inputs, **kwargs):
        mean, log_var = inputs
        return k.backend.random_normal(tf.shape(log_var)) * k.backend.exp(log_var / 2) + mean


class VariationalAE(tf.keras.Model):
    """
    Reduces the dimensions from the features generated by the convolutional network

    Parameters
    ----------
    trainX : training data
    num_feat :the number of features from the training dataset
    gpu : which gpu to use
    batch_size : training batch size
    epochs : number of epochs
    checkpoint_path : the path to store the training checkpoint
    training_num : the training instance number
    coding_size: units for the latent layer
    earlystop: monitors the validation loss and stops the training if there is no improvement
    verbose: Display training information
    validation_split: fraction to split dataset for validation
    scaling_factor: normalizes the data based on the input image size

    Returns
    -------
    """

    def __init__(self, trainX=None,
                 num_feat=42,
                 gpu=None,
                 batch_size=128,
                 epochs=10,
                 coding_size=8,
                 scaling_factor=10. * 10.,
                 checkpoint_path=None,
                 training_num=1,
                 earlystop=10,
                 validation_split=0.2,
                 verbose=1):
        super(VariationalAE, self).__init__()
        if checkpoint_path is None:
            print('Please provide a place to save checkpoints')
            return
        else:
            self.checkpoint_path = checkpoint_path
            self.training_num = training_num
            self.checkpoint_dir = f"{self.checkpoint_path}/training_{self.training_num}/"
            if not os.path.exists(self.checkpoint_dir):
                os.makedirs(self.checkpoint_dir)

        self.gpu = gpu
        if self.gpu is None:
            os.environ["CUDA_VISIBLE_DEVICES"] = '0'
            self.gpu = '0'
        elif isinstance(self.gpu, int):
            os.environ["CUDA_VISIBLE_DEVICES"] = str(self.gpu)
        else:
            os.environ["CUDA_VISIBLE_DEVICES"] = self.gpu

        self.trainX = trainX
        self.batch_size, self.epochs = batch_size, epochs
        self.num_feat = num_feat
        self.coding_size = coding_size
        self.scaling_factor = scaling_factor
        self.earlystop = earlystop
        self.validation_split = validation_split
        self.verbose = verbose

        self.model = self._model(self.coding_size)

    def _model(self, coding_size):

        # Building the encoder
        input_ = k.layers.Input(shape=[self.num_feat])
        enc = k.layers.Dense(256, activation='selu', name='Encoding1')(input_)
        batch_norm = k.layers.BatchNormalization(name='batchnorm1')(enc)
        drop = k.layers.Dropout(rate=0.2, name='dropout1')(batch_norm)
        enc = k.layers.Dense(128, activation='selu', name='Encoding2')(drop)
        batch_norm = k.layers.BatchNormalization(name='batchnorm2')(enc)
        drop = k.layers.Dropout(rate=0.2, name='dropout2')(batch_norm)
        enc = k.layers.Dense(64, activation='selu', name='Encoding3')(drop)
        batch_norm = k.layers.BatchNormalization(name='batchnorm3')(enc)
        drop = k.layers.Dropout(rate=0.2, name='dropout3')(batch_norm)
        enc = k.layers.Dense(32, activation='selu', name='Encoding4')(drop)

        # Variational part
        codings_mean = k.layers.Dense(coding_size)(enc)  # the mean
        codings_log_var = k.layers.Dense(coding_size)(enc)  # the log of the standard deviation
        codings = Sampling()([codings_mean, codings_log_var])
        self.variational_encoder = k.Model(inputs=[input_], outputs=[codings_mean, codings_log_var, codings])

        # Building the decoder
        decoder_inputs = k.layers.Input(shape=[coding_size])
        dec = k.layers.Dense(32, activation='selu', name='Decoding1')(decoder_inputs)
        drop = k.layers.Dropout(rate=0.2, name='dropout5')(dec)
        batch_norm = k.layers.BatchNormalization(name='batchnorm5')(drop)
        dec = k.layers.Dense(64, activation='selu', name='Decoding2')(batch_norm)
        drop = k.layers.Dropout(rate=0.2, name='dropout6')(dec)
        batch_norm = k.layers.BatchNormalization(name='batchnorm6')(drop)
        dec = k.layers.Dense(128, activation='selu', name='Decoding3')(batch_norm)
        drop = k.layers.Dropout(rate=0.2, name='dropout7')(dec)
        batch_norm = k.layers.BatchNormalization(name='batchnorm7')(drop)
        dec = k.layers.Dense(256, activation='selu', name='Decoding4')(batch_norm)
        drop = k.layers.Dropout(rate=0.2, name='dropout8')(dec)
        batch_norm = k.layers.BatchNormalization(name='batchnorm8')(drop)
        output_ = k.layers.Dense(self.num_feat, activation='linear', name='Decoding_final')(batch_norm)
        self.variational_decoder = k.Model(inputs=[decoder_inputs], outputs=[output_])

        _, _, codings = self.variational_encoder(input_)
        reconstructions = self.variational_decoder(codings)
        model = k.Model(inputs=[input_], outputs=[reconstructions])

        latent_loss = -0.5 * k.backend.sum(1 + codings_log_var - k.backend.exp(codings_log_var)
                                           - k.backend.square(codings_mean), axis=-1)

        model.add_loss(k.backend.mean(latent_loss) / self.scaling_factor)
        opt = k.optimizers.Adam(lr=0.001)
        loss_val = k.losses.MeanSquaredError()
        model.compile(loss=loss_val, optimizer=opt, metrics=['accuracy'])

        return model

    def train(self, trainX=None):

        if trainX is None:
            try:
                trainX = self.trainX
            except trainX.NotProvided:
                raise ValueError('Please provide trainX')

        #  Create a callback that saves the model's weights
        check_path = self.checkpoint_dir + 'cp-{epoch:04d}.ckpt'
        cp_callback = k.callbacks.ModelCheckpoint(filepath=check_path, verbose=1,
                                                  save_weights_only=True,
                                                  save_best_only=True)

        cp_earlystop = k.callbacks.EarlyStopping(patience=self.earlystop,
                                                 restore_best_weights=True)

        # Save the weights using the `checkpoint_path` format
        self.model.save_weights(check_path.format(epoch=0))

        history_vae = self.model.fit(trainX, trainX, epochs=self.epochs,
                                     batch_size=self.batch_size,
                                     validation_split=self.validation_split,
                                     callbacks=[cp_callback, cp_earlystop],
                                     verbose=self.verbose)

        return history_vae
